{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb278d04",
   "metadata": {},
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9400c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoobot.pytorch.training.finetune import FinetuneableZoobotClassifier\n",
    "from zoobot.pytorch.training.finetune import FinetuneableZoobotAbstract\n",
    "from galaxy_datasets.pytorch.galaxy_datamodule import CatalogDataModule\n",
    "\n",
    "from zoobot.pytorch.training.finetune import LinearHead\n",
    "import logging\n",
    "from functools import partial\n",
    "import torchmetrics as tm\n",
    "from zoobot.pytorch.training.finetune import cross_entropy_loss\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15025403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneableZoobotMetadataAbstract(FinetuneableZoobotAbstract):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def run_step_through_model(self, batch):\n",
    "        # part of training/val/test for all subclasses\n",
    "        image, y = self.batch_to_supervised_tuple(batch)\n",
    "        y_pred = self.forward(batch)\n",
    "             \n",
    "        # must be subclasses and specified\n",
    "        loss = self.loss(y_pred, y)  # type:ignore\n",
    "        loss.float()\n",
    "        return y, y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetuneableZoobotMetadataClassifier(FinetuneableZoobotMetadataAbstract, FinetuneableZoobotClassifier):    \n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            label_col: str = 'label',\n",
    "            label_smoothing=0.,\n",
    "            class_weights=None,\n",
    "            metadata_cols=None,\n",
    "\n",
    "            run_linear_sanity_check: bool = False,\n",
    "            **super_kwargs) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            num_classes=num_classes,\n",
    "            label_col=label_col,\n",
    "            label_smoothing=label_smoothing,\n",
    "            class_weights=class_weights,\n",
    "            **super_kwargs\n",
    "        )\n",
    "\n",
    "        self.label_col = label_col\n",
    "        \n",
    "        logging.info(\"Using classification head and cross-entropy loss\")\n",
    "        self.head = LinearHead(\n",
    "            input_dim=self.encoder_dim,  # type: ignore\n",
    "            output_dim=num_classes,\n",
    "            head_dropout_prob=self.head_dropout_prob,\n",
    "        )\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        # if isinstance(class_weights, list) or isinstance(class_weights, np.ndarray):\n",
    "        if class_weights is not None:\n",
    "            # https://lightning.ai/docs/pytorch/stable/accelerators/accelerator_prepare.html#init-tensors-using-tensor-to-and-register-buffer\n",
    "            self.register_buffer(\"class_weights\", torch.Tensor(class_weights))\n",
    "            print(self.class_weights, self.class_weights.device)  # type: ignore\n",
    "            # can now use self.class_weights in forward pass and will be on correct device (because treated as model parameters)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "        self.loss = partial(cross_entropy_loss,\n",
    "                            weight=self.class_weights,\n",
    "                            label_smoothing=self.label_smoothing)\n",
    "        logging.info(f'num_classes: {num_classes}')\n",
    "\n",
    "        if num_classes == 2:\n",
    "            logging.info(\"Using binary classification\")\n",
    "            task = \"binary\"\n",
    "        else:\n",
    "            logging.info(\"Using multi-class classification\")\n",
    "            task = \"multiclass\"\n",
    "        self.train_acc = tm.Accuracy(task=task, average=\"micro\", num_classes=num_classes)\n",
    "        self.val_acc = tm.Accuracy(task=task, average=\"micro\", num_classes=num_classes)\n",
    "        self.test_acc = tm.Accuracy(task=task, average=\"micro\", num_classes=num_classes)\n",
    "\n",
    "        self.run_linear_sanity_check = run_linear_sanity_check\n",
    "        \n",
    "        self.metadata_cols = metadata_cols or []\n",
    "        metadata_dim = len(self.metadata_cols)\n",
    "        \n",
    "        prev_head = self.head\n",
    "        self.head = LinearHead(\n",
    "            input_dim=prev_head.input_dim + metadata_dim,\n",
    "            output_dim=num_classes,\n",
    "            head_dropout_prob=prev_head.dropout.p,\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        x = torch.tensor(batch['image'], dtype=torch.float, device=self.device)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # collect metadata columns as tensor\n",
    "        if self.metadata_cols:\n",
    "            metadata = torch.cat([\n",
    "                torch.tensor(batch[col], dtype=torch.float, device=x.device).unsqueeze(1)\n",
    "                for col in self.metadata_cols\n",
    "            ], dim=1)\n",
    "            x = torch.cat([x, metadata], dim=1)\n",
    "\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27145e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv('../../imgs/train_dataset.csv')\n",
    "test_metadata = pd.read_csv('../../imgs/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389bc3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>id_str</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>a17e8ab7a7de9c79d3cf960af591bfd113669e405a9803...</td>\n",
       "      <td>3.955509</td>\n",
       "      <td>9.051425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>1</td>\n",
       "      <td>2acac60ba3f744afb21f2e6c59d876949c8305e4f519cf...</td>\n",
       "      <td>9.231839</td>\n",
       "      <td>23.476162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>50f422c99ac53ecdfb5559fb71e758c7052f3f4ad74e58...</td>\n",
       "      <td>1.518943</td>\n",
       "      <td>2.276750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>12fba589f6f7b0a3ce5c17039760b3c98e1cd6b69a9c1f...</td>\n",
       "      <td>9.937435</td>\n",
       "      <td>25.216443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>1</td>\n",
       "      <td>f69e6578faa676168e96e9e4595fb79fff28aee64fd6bf...</td>\n",
       "      <td>5.620438</td>\n",
       "      <td>16.640429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label  \\\n",
       "0  <PIL.JpegImagePlugin.JpegImageFile image mode=...      0   \n",
       "1  <PIL.JpegImagePlugin.JpegImageFile image mode=...      1   \n",
       "2  <PIL.JpegImagePlugin.JpegImageFile image mode=...      0   \n",
       "3  <PIL.JpegImagePlugin.JpegImageFile image mode=...      0   \n",
       "4  <PIL.JpegImagePlugin.JpegImageFile image mode=...      1   \n",
       "\n",
       "                                              id_str         X          y  \n",
       "0  a17e8ab7a7de9c79d3cf960af591bfd113669e405a9803...  3.955509   9.051425  \n",
       "1  2acac60ba3f744afb21f2e6c59d876949c8305e4f519cf...  9.231839  23.476162  \n",
       "2  50f422c99ac53ecdfb5559fb71e758c7052f3f4ad74e58...  1.518943   2.276750  \n",
       "3  12fba589f6f7b0a3ce5c17039760b3c98e1cd6b69a9c1f...  9.937435  25.216443  \n",
       "4  f69e6578faa676168e96e9e4595fb79fff28aee64fd6bf...  5.620438  16.640429  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8edd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata[\"file_loc\"] = \"../../imgs/images/train/\" + train_metadata[\"id_str\"].astype(str) + \".jpg\"\n",
    "test_metadata[\"file_loc\"] = \"../../imgs/images/test/\" + test_metadata[\"id_str\"].astype(str) + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b197f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>id_str</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "      <th>file_loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>a17e8ab7a7de9c79d3cf960af591bfd113669e405a9803...</td>\n",
       "      <td>3.955509</td>\n",
       "      <td>9.051425</td>\n",
       "      <td>../../imgs/images/train/a17e8ab7a7de9c79d3cf96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>1</td>\n",
       "      <td>2acac60ba3f744afb21f2e6c59d876949c8305e4f519cf...</td>\n",
       "      <td>9.231839</td>\n",
       "      <td>23.476162</td>\n",
       "      <td>../../imgs/images/train/2acac60ba3f744afb21f2e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>50f422c99ac53ecdfb5559fb71e758c7052f3f4ad74e58...</td>\n",
       "      <td>1.518943</td>\n",
       "      <td>2.276750</td>\n",
       "      <td>../../imgs/images/train/50f422c99ac53ecdfb5559...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>0</td>\n",
       "      <td>12fba589f6f7b0a3ce5c17039760b3c98e1cd6b69a9c1f...</td>\n",
       "      <td>9.937435</td>\n",
       "      <td>25.216443</td>\n",
       "      <td>../../imgs/images/train/12fba589f6f7b0a3ce5c17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>1</td>\n",
       "      <td>f69e6578faa676168e96e9e4595fb79fff28aee64fd6bf...</td>\n",
       "      <td>5.620438</td>\n",
       "      <td>16.640429</td>\n",
       "      <td>../../imgs/images/train/f69e6578faa676168e96e9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label  \\\n",
       "0  <PIL.JpegImagePlugin.JpegImageFile image mode=...      0   \n",
       "1  <PIL.JpegImagePlugin.JpegImageFile image mode=...      1   \n",
       "2  <PIL.JpegImagePlugin.JpegImageFile image mode=...      0   \n",
       "3  <PIL.JpegImagePlugin.JpegImageFile image mode=...      0   \n",
       "4  <PIL.JpegImagePlugin.JpegImageFile image mode=...      1   \n",
       "\n",
       "                                              id_str         X          y  \\\n",
       "0  a17e8ab7a7de9c79d3cf960af591bfd113669e405a9803...  3.955509   9.051425   \n",
       "1  2acac60ba3f744afb21f2e6c59d876949c8305e4f519cf...  9.231839  23.476162   \n",
       "2  50f422c99ac53ecdfb5559fb71e758c7052f3f4ad74e58...  1.518943   2.276750   \n",
       "3  12fba589f6f7b0a3ce5c17039760b3c98e1cd6b69a9c1f...  9.937435  25.216443   \n",
       "4  f69e6578faa676168e96e9e4595fb79fff28aee64fd6bf...  5.620438  16.640429   \n",
       "\n",
       "                                            file_loc  \n",
       "0  ../../imgs/images/train/a17e8ab7a7de9c79d3cf96...  \n",
       "1  ../../imgs/images/train/2acac60ba3f744afb21f2e...  \n",
       "2  ../../imgs/images/train/50f422c99ac53ecdfb5559...  \n",
       "3  ../../imgs/images/train/12fba589f6f7b0a3ce5c17...  \n",
       "4  ../../imgs/images/train/f69e6578faa676168e96e9...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db98e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galaxy_datasets.transforms import default_view_config, get_galaxy_transform\n",
    "\n",
    "transform_cfg = default_view_config()\n",
    "transform = get_galaxy_transform(transform_cfg)\n",
    "\n",
    "datamodule = CatalogDataModule(\n",
    "  label_cols=[\"label\", \"X\"],\n",
    "  catalog=train_metadata,\n",
    "  train_transform=transform,\n",
    "  test_transform=transform,\n",
    "  batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetuneableZoobotMetadataClassifier(\n",
    "    name='hf_hub:mwalmsley/zoobot-encoder-efficientnet_b0',\n",
    "    training_mode=\"full\",\n",
    "    learning_rate=5e-5,\n",
    "    layer_decay=0.65,\n",
    "    num_classes=2,\n",
    "    label_col='label',\n",
    "    metadata_cols=['X']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de03b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearHead(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=1281, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafe91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type           | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | encoder           | EfficientNet   | 4.0 M  | train\n",
      "1 | train_loss_metric | MeanMetric     | 0      | train\n",
      "2 | val_loss_metric   | MeanMetric     | 0      | train\n",
      "3 | test_loss_metric  | MeanMetric     | 0      | train\n",
      "4 | head              | LinearHead     | 2.6 K  | train\n",
      "5 | train_acc         | BinaryAccuracy | 0      | train\n",
      "6 | val_acc           | BinaryAccuracy | 0      | train\n",
      "7 | test_acc          | BinaryAccuracy | 0      | train\n",
      "-------------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.040    Total estimated model params size (MB)\n",
      "346       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5a6f922d4a438bbea92f30e6864c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anna\\Downloads\\Projects\\NeuralNetworks_KMA\\lab2 and 3\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Anna\\AppData\\Local\\Temp\\ipykernel_8000\\3730520974.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(batch['image'], dtype=torch.float, device=self.device)\n",
      "C:\\Users\\Anna\\AppData\\Local\\Temp\\ipykernel_8000\\3730520974.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(batch[col], dtype=torch.float, device=x.device).unsqueeze(1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5abe0cb5b8a41758b80a2eec5c03594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d87a7127574b64b2f51f7015654dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 176: 'finetuning/val_loss' reached 0.55278 (best 0.55278), saving model to 'C:\\\\Users\\\\Anna\\\\Downloads\\\\Projects\\\\NeuralNetworks_KMA\\\\lab2 and 3\\\\zoobot_metadata\\\\zoobot\\\\zoobot_metadata\\\\save_dir\\\\checkpoints\\\\0.ckpt' as top 1\n"
     ]
    }
   ],
   "source": [
    "from zoobot.pytorch.training.finetune import get_trainer\n",
    "\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "save_dir = os.path.join('./save_dir/')\n",
    "\n",
    "trainer = get_trainer(save_dir, accelerator=\"auto\", devices=1, strategy=\"auto\", max_epochs=10)\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed89fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = FinetuneableZoobotClassifier.load_from_checkpoint('./zoobot_finetune/checkpoints/6.ckpt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
